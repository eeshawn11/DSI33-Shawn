{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brkit\\Anaconda3\\envs\\mediapipe\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util, config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import cv2\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe_model_maker import gesture_recognizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Naruto Hand Seals\n",
    "\n",
    "---\n",
    "\n",
    "Anime and manga were a big part of my daily entertainment while growing up as a 90s kid, in particular the Big 3 - One Piece, Naruto and Bleach. While One Piece still remains my personal favourite (and still ongoing as of 2023!), today's focus will be on [**Naruto**](https://naruto.fandom.com/wiki/Narutopedia), which tells the story of a young ninja called Uzumaki Naruto trying to achieve his dream of becoming the Hokage.\n",
    "\n",
    "One of the fundamental concepts within Naruto is the use of chakra, which in turn allows a user to perform a jutsu (technique). Through the use of hand seals, a ninja can better control and manipulate their chakra when performing their technique. There are twelve basic seals, each of them named after an animal in the Chinese Zodiac. There are different sequences of hand seals for every technique, but a skilled ninja could also use less or no hand seals in order to perform a technique.\n",
    "\n",
    "![Twelve Basic Hand Seals](Naruto_Hand_Seals_by_Megan.gif)\n",
    "<br>[Source: Naruto Hand Seals by Megan #1](https://www.youtube.com/watch?v=y_NRTgVuaNo)\n",
    "\n",
    "While I am not savvy enough to perform hand seals with the same accuracy and speed as [Megan](https://www.youtube.com/@DreamSilver05) in the image above, I wondered how the new Python and deep learning skills I've picked up recently could be applied. This is by no means an original project and I've also come across and referenced various other enthusiasts who have developed their own computer vision models. However, this is a great opportunity for me to practise using some of the various deep learning and computer vision libraries, like TensorFlow and OpenCV, that I did not have much opportunities to interact with during my Data Science Immersive.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Using `Object Detection`, we will attempt to train a model that can recognise the 12 basic hand seals in a live video feed with XXX accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Referencing the [TensorFlow Object Detection Tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html), I will be using transfer learning from a pre-trained network to customise it to our current task.\n",
    "\n",
    "We will prepare our workspace as recommended in the tutorial and assign our file paths to constants for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up paths for easier reference\n",
    "\n",
    "SCRIPTS_PATH = '../../scripts/preprocessing'\n",
    "# APIMODEL_PATH = '../../models'\n",
    "ANNOTATION_PATH = './annotations'\n",
    "IMAGE_PATH = './images'\n",
    "COLLECTION_PATH = IMAGE_PATH + '/collected'\n",
    "MODEL_PATH = './models/my_ssd_mobilenet_v2_fpnlite'\n",
    "LOG_PATH = MODEL_PATH + '/train'\n",
    "# PRETRAINED_MODEL_PATH = './pre-trained-models'\n",
    "CONFIG_PATH = MODEL_PATH + '/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Label Maps\n",
    "\n",
    "---\n",
    "\n",
    "We will assign a label name and id to each of the 12 hand seals, creating a label map that TensorFlow will use in the training and detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    {'name':'rat', 'id':1}, \n",
    "    {'name':'ox', 'id':2},\n",
    "    {'name':'tiger', 'id':3},\n",
    "    {'name':'hare', 'id':4},\n",
    "    {'name':'dragon', 'id':5},\n",
    "    {'name':'snake', 'id':6},\n",
    "    {'name':'horse', 'id':7},\n",
    "    {'name':'ram', 'id':8},\n",
    "    {'name':'monkey', 'id':9},\n",
    "    {'name':'bird', 'id':10},\n",
    "    {'name':'dog', 'id':11},\n",
    "    {'name':'boar', 'id':12},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ANNOTATION_PATH + '/label_map.pbtxt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Images\n",
    "\n",
    "---\n",
    "\n",
    "While I'm not the first to attempt such a project, I did not come across any existing datasets online, so this would be a great opportunity to ~~toy around~~ practise building my own dataset.\n",
    "\n",
    "I will utilise OpenCV to capture some images of myself in various lighting conditions, as well as search online for a mixture of anime/manga and real-life samples, which should hopefully provide more generalisability to the model. We will target for approximately 200 samples per hand seal, which can then be split into our train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_images(label: str, number_images: int = 20):\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    print(f'Starting collection of images for {label} in 3 seconds')\n",
    "    time.sleep(3)\n",
    "    for image_number in range(number_images):\n",
    "        image_name = os.path.join(COLLECTION_PATH, f'{label}_{image_number}_{uuid.uuid4().hex}.jpg')\n",
    "        print(f'Capturing {image_name}')\n",
    "        time.sleep(1)\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imwrite(image_name, frame)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare for rat, starting in in 5 seconds\n",
      "Starting collection of images for rat in 3 seconds\n",
      "Capturing ./images/collected\\rat_0_0b163023493c4f2fb0e64250aeb8f7e7.jpg\n",
      "Capturing ./images/collected\\rat_1_96c6561886454734bad1783eb3047be7.jpg\n",
      "Capturing ./images/collected\\rat_2_02ad6ccf1c744342b2f133b6590e9637.jpg\n",
      "Capturing ./images/collected\\rat_3_3ebe7da4bcbf4f47bce28f104e4412c6.jpg\n",
      "Capturing ./images/collected\\rat_4_9a283f6671774fa7ba77ddd54702608e.jpg\n",
      "Prepare for ox, starting in in 5 seconds\n",
      "Starting collection of images for ox in 3 seconds\n",
      "Capturing ./images/collected\\ox_0_a73b75f7a7f5430291b450e29205e399.jpg\n",
      "Capturing ./images/collected\\ox_1_e329dc1477014cd99afa7edf317cf461.jpg\n",
      "Capturing ./images/collected\\ox_2_e29f045adb114b94875e7bf89a9be1e8.jpg\n",
      "Capturing ./images/collected\\ox_3_f348f269c9b64ba7bfe86ab3cccd8aa8.jpg\n",
      "Capturing ./images/collected\\ox_4_d1cf79c45eb047259feb02e00ea831dd.jpg\n",
      "Prepare for tiger, starting in in 5 seconds\n",
      "Starting collection of images for tiger in 3 seconds\n",
      "Capturing ./images/collected\\tiger_0_3ee6c3859f66464dbb554a0882bdc832.jpg\n",
      "Capturing ./images/collected\\tiger_1_3e28474efb3942fea3169384937e7bb3.jpg\n",
      "Capturing ./images/collected\\tiger_2_f400ad742ff7416db038780336d095b2.jpg\n",
      "Capturing ./images/collected\\tiger_3_75a9e3ed7fdc4e7eae3b003519ac233b.jpg\n",
      "Capturing ./images/collected\\tiger_4_67e03b03d5144ab299309799e1a3bc3c.jpg\n",
      "Prepare for hare, starting in in 5 seconds\n",
      "Starting collection of images for hare in 3 seconds\n",
      "Capturing ./images/collected\\hare_0_88cf36f7fe8541d4891444d915c427e4.jpg\n",
      "Capturing ./images/collected\\hare_1_d7d165aa5c784d26ac57db837316338f.jpg\n",
      "Capturing ./images/collected\\hare_2_15ad38d84d884704bcd3a8d4603b21bd.jpg\n",
      "Capturing ./images/collected\\hare_3_0fdd78858beb4948964d525df8cca1a6.jpg\n",
      "Capturing ./images/collected\\hare_4_5c7bdb473a31471090fc8ed69ccb3a1d.jpg\n",
      "Prepare for dragon, starting in in 5 seconds\n",
      "Starting collection of images for dragon in 3 seconds\n",
      "Capturing ./images/collected\\dragon_0_da955805d05b43639ef5e98017c8b157.jpg\n",
      "Capturing ./images/collected\\dragon_1_4e1255fa9aed43bba4cdede21317a85e.jpg\n",
      "Capturing ./images/collected\\dragon_2_d500551bdbd64f0db266084e0c973d54.jpg\n",
      "Capturing ./images/collected\\dragon_3_cfff895f838c465f90b16822383059f0.jpg\n",
      "Capturing ./images/collected\\dragon_4_cc1f3008a638424a801cc80c6a10c1f1.jpg\n",
      "Prepare for snake, starting in in 5 seconds\n",
      "Starting collection of images for snake in 3 seconds\n",
      "Capturing ./images/collected\\snake_0_b9374d4b20ab412cb35ded898016277d.jpg\n",
      "Capturing ./images/collected\\snake_1_a18fc0f60509451fbf86da807d0ac3d5.jpg\n",
      "Capturing ./images/collected\\snake_2_6005fbe14fa44b308115bc16cc7d5971.jpg\n",
      "Capturing ./images/collected\\snake_3_4fa411d2a91f45509fa25df8350bc926.jpg\n",
      "Capturing ./images/collected\\snake_4_eb87446d789c48feaabb63f2aeb49c2e.jpg\n",
      "Prepare for horse, starting in in 5 seconds\n",
      "Starting collection of images for horse in 3 seconds\n",
      "Capturing ./images/collected\\horse_0_9501f5761f6d4ddbbab77882ecc4d4be.jpg\n",
      "Capturing ./images/collected\\horse_1_1fc50f29a8b44945becb95424f7bb3a7.jpg\n",
      "Capturing ./images/collected\\horse_2_a3d3758217ba443ebf33e7a457aed28b.jpg\n",
      "Capturing ./images/collected\\horse_3_68f6c30f6ff743e496d1c2fbc33092f0.jpg\n",
      "Capturing ./images/collected\\horse_4_20cdb2f08b2e4dd59262a7154278f8c3.jpg\n",
      "Prepare for ram, starting in in 5 seconds\n",
      "Starting collection of images for ram in 3 seconds\n",
      "Capturing ./images/collected\\ram_0_11c9d3cce0d54536a2624bae8f431d8f.jpg\n",
      "Capturing ./images/collected\\ram_1_4b0efa4933844d3683b682008147e9f4.jpg\n",
      "Capturing ./images/collected\\ram_2_ecd7ab08b65846b8bd338ecab34f24e2.jpg\n",
      "Capturing ./images/collected\\ram_3_4dba9381e7db4328a218ac497d1061fd.jpg\n",
      "Capturing ./images/collected\\ram_4_72a80248c1d642648e5fd437eaa5a878.jpg\n",
      "Prepare for monkey, starting in in 5 seconds\n",
      "Starting collection of images for monkey in 3 seconds\n",
      "Capturing ./images/collected\\monkey_0_89ce556ad10c4ab28868a959be242b34.jpg\n",
      "Capturing ./images/collected\\monkey_1_b26a625d00cf42499c0bd9db540533b5.jpg\n",
      "Capturing ./images/collected\\monkey_2_f4544d27280046b984a38be460b426ce.jpg\n",
      "Capturing ./images/collected\\monkey_3_1c45df973d2d4c65b0367ab88c0b6743.jpg\n",
      "Capturing ./images/collected\\monkey_4_0bea02fbdb37404c83881825482a8d05.jpg\n",
      "Prepare for bird, starting in in 5 seconds\n",
      "Starting collection of images for bird in 3 seconds\n",
      "Capturing ./images/collected\\bird_0_fed6365228434e708a4c04beef62a3f7.jpg\n",
      "Capturing ./images/collected\\bird_1_99850400feee4bcf960293b24bbafd86.jpg\n",
      "Capturing ./images/collected\\bird_2_2f91cca509c7448c8a3da0a4c78e77b2.jpg\n",
      "Capturing ./images/collected\\bird_3_07d87b05b2a24e8b92e68151aa716127.jpg\n",
      "Capturing ./images/collected\\bird_4_e795a346d1cf412382b900dae32b07b8.jpg\n",
      "Prepare for dog, starting in in 5 seconds\n",
      "Starting collection of images for dog in 3 seconds\n",
      "Capturing ./images/collected\\dog_0_eb76206e27114163b0494a8d63eb834b.jpg\n",
      "Capturing ./images/collected\\dog_1_8281ce79100c43d285794b9b30a01cfe.jpg\n",
      "Capturing ./images/collected\\dog_2_8040bfad33d2408ca42085c8a9e10cfd.jpg\n",
      "Capturing ./images/collected\\dog_3_d9d04c9cdf45456e8d55c65603d5eee8.jpg\n",
      "Capturing ./images/collected\\dog_4_9e1504e6cf71437cafcc68fa15f2f402.jpg\n",
      "Prepare for boar, starting in in 5 seconds\n",
      "Starting collection of images for boar in 3 seconds\n",
      "Capturing ./images/collected\\boar_0_e8500327faf24563a04d6af07219e0ff.jpg\n",
      "Capturing ./images/collected\\boar_1_2ec43255ada845a8838fc38d85f73422.jpg\n",
      "Capturing ./images/collected\\boar_2_0898c9cab4924a6486a11743274133b4.jpg\n",
      "Capturing ./images/collected\\boar_3_b63180f64e3448b180ef4708297cae74.jpg\n",
      "Capturing ./images/collected\\boar_4_0f85913ff90d449c80b6b93ba5cd3a74.jpg\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(f\"Prepare for {label['name']}, starting in in 5 seconds\")\n",
    "    time.sleep(5)\n",
    "    capture_images(label['name'], number_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train records: 884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sign\n",
       "ram       86\n",
       "tiger     79\n",
       "snake     76\n",
       "bird      75\n",
       "rat       73\n",
       "dragon    72\n",
       "ox        71\n",
       "horse     71\n",
       "hare      71\n",
       "monkey    70\n",
       "dog       70\n",
       "boar      70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of files generated\n",
    "annotation_list = []\n",
    "\n",
    "for annotation in os.listdir(IMAGE_PATH+'/train'):\n",
    "    if \".xml\" not in annotation:\n",
    "        annotation_list.append(annotation.split('.')[0])\n",
    "\n",
    "df = pd.DataFrame(annotation_list, columns=['image'])\n",
    "df[['sign', 'image']] = df['image'].str.split('_', expand=True, n=1)\n",
    "print(f\"Total train records: {len(df)}\")\n",
    "df.value_counts('sign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection of images for rat in 3 seconds\n",
      "Capturing ./images/collected\\rat_0_3061bb7957d84da38c56959149047e96.jpg\n"
     ]
    }
   ],
   "source": [
    "capture_images('rat', 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation with LabelImg\n",
    "\n",
    "Using LabelImg, I went through the various images and annotated them to define the bounding boxes and labels that our model will eventually learn to recognise. \n",
    "\n",
    "During the first attempt to train the model, the model was not able to perform very effectively. After some evaluation of my workflow, I realised that my bounding boxes were not tight enough and I was leaving too much empty space. Since I am more concerned with trying to recognise the type of hand seal rather than trying to create an accurate bounding box on screen, I opted to \"zoom in\" to the key features of each hand seal to hopefully help the model perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Labelimg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: ./annotations/train.record\n",
      "Successfully created the TFRecord file: ./annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir ./models/my_ssd_mobilenet_v2_fpnlite/train\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensorboard --logdir {LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python model_main_tf2.py --model_dir=./models/my_ssd_mobilenet_v2_fpnlite --pipeline_config_path=./models/my_ssd_mobilenet_v2_fpnlite/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "# create statement to paste into command line\n",
    "print(f'python model_main_tf2.py --model_dir={MODEL_PATH} --pipeline_config_path={CONFIG_PATH}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline config and build model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-13')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Real Time Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup CV2 capture\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int32)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_detections,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+label_id_offset,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=1,\n",
    "        min_score_thresh=0.85,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    cv2.imshow('Naruto Hand Seal Detection', cv2.resize(image_np_with_detections, (800, 600)))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to be performing relatively well in generally identifying the models, but there are a few hand seals that are harder for the model to distinguish, or work only at certain angles. This could perhaps be overcome by providing more training samples are slightly different angles rather than only straight on to the camera.\n",
    "\n",
    "In particular, the "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilising Mediapipe Hands\n",
    "\n",
    "\n",
    "https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer/customize\n",
    "\n",
    "The dataset for gesture recognition in model maker requires the following format: <dataset_path>/<label_name>/<img_name>.*. In addition, one of the label names (label_names) must be none. The none label represents any gesture that isn't classified as one of the other gestures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/Users/brkit/Documents/DSI33-Shawn/Tensorflow/workspace/training_naruto/pre-trained-models/mediapipe_hand_landmarker/hand_landmarker.task'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gesture_recognizer.Dataset.from_folder(\n",
    "    dirname=dataset_path,\n",
    "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
    ")\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
    "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
    "model = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data, batch_size=1)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74bea083df7b4d0af8215e8865fec83670090328e0448732258baf3c2a2d7d5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
